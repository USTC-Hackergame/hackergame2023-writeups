import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer

theme_kargs = {
    "font": ["Helvetica", "ui-sans-serif", "system-ui", "sans-serif"],
    "font_mono": ["IBM Plex Mono", "ui-monospace", "Consolas", "monospace"],
}

custom_css = """
.svelte-1pjfiar {
    -webkit-user-select: text;
    -moz-user-select: text;
    -ms-user-select: text;
    user-select: text;
    cursor: auto !important;
}
"""

model = AutoModelForCausalLM.from_pretrained(
    "roneneldan/TinyStories-33M").eval()
tokenizer = AutoTokenizer.from_pretrained("roneneldan/TinyStories-33M")

HINT = """å’Œæˆ‘èŠå¤©å¹¶ä¸”è®©æˆ‘è¯´å‡ºç‰¹å®šçš„è¯è¯­ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰è¯æ˜ä½ è¶³å¤Ÿèªæ˜ ğŸ¤”ã€‚

è®©æˆ‘è¯´å‡º `you are smart` æˆ‘ä¼šé€ç»™ä½  flag1ï¼›
è®©æˆ‘è¯´å‡º `accepted` æˆ‘ä¼šé€ç»™ä½  flag2ï¼ˆæ¶ˆæ¯é•¿åº¦ä¸è¶…è¿‡ 7ï¼‰ï¼›
è®©æˆ‘è¯´å‡º `hackergame` æˆ‘ä¼šé€ç»™ä½  flag3ï¼ˆæ¶ˆæ¯é•¿åº¦ä¸è¶…è¿‡ 100ï¼‰ï¼›
è®©æˆ‘è¯´å‡º `ğŸ®` æˆ‘ä¼šé€ç»™ä½  flag4ï¼ˆæ¶ˆæ¯é•¿åº¦ä¸è¶…è¿‡ 200ï¼‰ã€‚

ç°åœ¨è®©æˆ‘ä»¬å¼€å§‹å§ï¼ğŸ”¥"""


def check_message(message):
    if len(message) > 200:
        return False, "[*] å‡ºé”™äº†ï¼Œæ¶ˆæ¯é•¿åº¦ä¸èƒ½è¶…è¿‡ 200ã€‚"
    return True, None


def check_response(message, response):
    response = response.strip().lower()
    info = ""
    flag1, flag2, flag3, flag4 = 'flag{fake_flag1}', 'flag{fake_flag2}', 'flag{fake_flag3}', 'flag{fake_flag4}'
    if "you are smart" in response:
        info += f"\nğŸ‘ğŸ‘ğŸ‘ flag1: `{flag1}` ğŸ‘ğŸ‘ğŸ‘"
    if len(message) <= 7 and "accepted" in response:
        info += f"\nğŸ‰ğŸ‰ğŸ‰ flag2: `{flag2}` ğŸ‰ğŸ‰ğŸ‰"
    if len(message) <= 100 and "hackergame" in response:
        info += f"\nğŸŠğŸŠğŸŠ flag3: `{flag3}` ğŸŠğŸŠğŸŠ"
    if len(message) <= 200 and "ğŸ®" in response:
        info += f"\nğŸ”¥ğŸ”¥ğŸ”¥ flag4: `{flag4}` ğŸ”¥ğŸ”¥ğŸ”¥"
    return info


def predict(message):
    model_inputs = tokenizer.encode(message, return_tensors="pt")
    model_outputs = model.generate(
        model_inputs,
        max_new_tokens=30,
        num_beams=1,
        pad_token_id=tokenizer.eos_token_id,
    )
    model_outputs = model_outputs[0, len(model_inputs[0]):]
    model_output_text = tokenizer.decode(
        model_outputs, skip_special_tokens=True)
    return model_output_text


def bot(message, _history):
    ok, info = check_message(message)
    if not ok:
        return info

    try:
        model_output_text = predict(message)
    except Exception:
        return "å‡ºé”™äº†ï¼Œè¯·é‡æ–°å°è¯•ã€‚"

    info = check_response(message, model_output_text)
    if info:
        model_output_text += info

    return model_output_text


with gr.Blocks(theme=gr.themes.Default(**theme_kargs), css=custom_css) as demo:
    # Token for hackergame
    demo.load(None, [], [])

    #
    # Chatbot
    #
    chat = gr.ChatInterface(bot)
    source_code = gr.Code(
        value=open(__file__).read(), language="python", label="main.py"
    )
    demo.load(
        lambda: ([(None, HINT)], [(None, HINT)]), [], [
            chat.chatbot_state, chat.chatbot]
    )

if __name__ == "__main__":
    demo.queue().launch(show_api=False, share=False)
